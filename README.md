### The sections are as follows:
- [Large Language Model](#Large-Language-Model)
- [In-context Learning](#In-context-Learning)
- [Text Embeddings](#Text-Embeddings)


## Large Language Model
### 2023
- arXiv, Meta [Code Llama: Open Foundation Models for Code](https://arxiv.org/pdf/2308.12950v2.pdf)
- arXiv, Meta [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)
- ICML, EleutherAI [Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling](https://arxiv.org/abs/2304.01373) [[Code]](https://github.com/EleutherAI/pythia)

### 2022
- arXiv, OpenAI [Efficient Training of Language Models to Fill in the Middle](https://arxiv.org/abs/2207.14255)

## In-context Learning
### 2023
- ICML [Compositional Exemplars for In-context Learning](https://arxiv.org/abs/2302.05698)
- arXiv [Learning to Retrieve In-Context Examples for Large Language Models](https://arxiv.org/abs/2307.07164)

### 2022
- NAACL [Learning To Retrieve Prompts for In-Context Learning](https://arxiv.org/abs/2112.08633)


## Text Embeddings

### 2023
- ACL [One Embedder, Any Task: Instruction-Finetuned Text Embeddings](https://arxiv.org/abs/2212.09741)
- OpenAI [Text and Code Embeddings by Contrastive Pre-Training](https://cdn.openai.com/papers/Text_and_Code_Embeddings_by_Contrastive_Pre_Training.pdf)

### 2022
- arXiv, MS [Text Embeddings by Weakly-Supervised Contrastive Pre-training](https://arxiv.org/abs/2212.03533)
